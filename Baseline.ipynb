{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils\n",
    "from timm.data.loader import MultiEpochsDataLoader\n",
    "\n",
    "from model import *\n",
    "from data_loader import LoadDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "data_cfgs = {\"name\": \"DL20\", \"num_classes\": 20, \"dir\":\"./data/DL20\"}\n",
    "train_cfgs = {\"batch_size\": 32, \"lr\": 0.0001}\n",
    "\n",
    "### load small version of ResNet\n",
    "#model = Small_ResNet(BasicBlock, [3, 3, 3], num_classes=data_cfgs['num_classes']).to('cuda')\n",
    "#model = models.resnext101_32x8d(pretrained=True).to('cuda')\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0',num_classes = 20)\n",
    "#model._fc = nn.Sequential( \n",
    "#    nn.Linear(1280, 512, bias=True),\n",
    "#    nn.Linear(512, 20, bias=True))\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = True\n",
    "model = model.to('cuda')\n",
    "\n",
    "### load train/valid/test dataset\n",
    "train_dataset = LoadDataset(data_cfgs[\"dir\"], mode=\"train\", random_flip=True)\n",
    "valid_dataset = LoadDataset(data_cfgs[\"dir\"], mode=\"valid\", random_flip=False)\n",
    "#test_dataset = LoadDataset(data_cfgs[\"dir\"], mode=\"test\", random_flip=False)\n",
    "\n",
    "### warp dataset using dataloader\n",
    "train_dataloader = MultiEpochsDataLoader(train_dataset, batch_size=train_cfgs[\"batch_size\"], shuffle=True, pin_memory=True, drop_last=True, num_workers=64)\n",
    "valid_dataloader = MultiEpochsDataLoader(valid_dataset, batch_size=train_cfgs[\"batch_size\"], shuffle=False, pin_memory=True, drop_last=False, num_workers=64)\n",
    "#test_dataloader = MultiEpochsDataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True, drop_last=False)\n",
    "\n",
    " ### define Adam optimizer: one of the popular optimizers in Deep Learning community\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), train_cfgs[\"lr\"], eps=1e-6)\n",
    "\n",
    "### define cross-entropy loss for classification\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"NLL loss with label smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.0):\n",
    "        \"\"\"Constructor for the LabelSmoothing module.\n",
    "        :param smoothing: label smoothing factor\n",
    "        \"\"\"\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "#criterion = LabelSmoothing(smoothing = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gaussian37.github.io/dl-pytorch-lr_scheduler/ 에서 봄\n",
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "        self.T_cur = last_epoch\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "#scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=150, T_mult=1, eta_max=0.1,  T_up=10, gamma=0.5)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, patience, num_epochs):\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    mean_train_losses = []\n",
    "    mean_valid_losses = [] \n",
    "    p = 0\n",
    "    min_valid_loss = 100\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "        model.train() \n",
    "        for train_data, target in train_dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda\")\n",
    "                train_data, target = train_data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            #scheduler.step() #내가 추가한거\n",
    "\n",
    "        model.eval() \n",
    "        for valid_data, target in valid_dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda\")\n",
    "                valid_data, target = valid_data.to(device), target.to(device)\n",
    "            output = model(valid_data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        mean_train_losses.append(train_loss)\n",
    "        mean_valid_losses.append(valid_loss)\n",
    "        \n",
    "        if min_valid_loss > valid_loss:\n",
    "            min_valid_loss = valid_loss\n",
    "            # print(f'min_valid_loss: {min_valid_loss}')\n",
    "\n",
    "        epoch_len = len(str(num_epochs))\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        print(print_msg)\n",
    "        \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        print_test_accuracy(model, valid_dataloader) # 여따 넣어도 되나\n",
    "        \n",
    "        if min_valid_loss < valid_loss and epoch > 1:\n",
    "            p = p + 1\n",
    "            #print(f'patience: {p}')\n",
    "        else:\n",
    "            p = 0\n",
    "            torch.save(model.state_dict(), 'bestmodel.pt')\n",
    "            print(\"Saving Model...\")\n",
    "        \n",
    "        if patience == p:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load('bestmodel.pt'))\n",
    "\n",
    "    return  model, mean_train_losses, mean_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Test.csv… …\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-828bc8884895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtest_csv_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-828bc8884895>\u001b[0m in \u001b[0;36mtest_csv_file\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_names\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0meach_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "def test_csv_file(model):\n",
    "    print(\"Saving Test.csv… …\")\n",
    "\n",
    "    model.load_state_dict(torch.load('bestmodel.pt'))\n",
    "    model.eval()\n",
    "    arr = []\n",
    "    with torch.no_grad():\n",
    "        for images, file_names in iter(test_dataloader):\n",
    "            each_arr = []\n",
    "            images = images.to('cuda')\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            each_arr.append(file_names[0])\n",
    "            each_arr.append(int(predicted))\n",
    "            arr.append(each_arr)\n",
    "\n",
    "    arr = np.array(arr)\n",
    "    df = pd.DataFrame(arr, columns=['Id', 'Category'])\n",
    "    df.to_csv('test.csv', index=False)\n",
    "\n",
    "test_csv_file(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "def print_test_accuracy(model, test_loader):\n",
    "  class_correct = list(0. for i in range(20))\n",
    "  class_total = list(0. for i in range(20))\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for test_data, target in test_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            test_data, target = test_data.to(device), target.to(device)\n",
    "        output = model(test_data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        c = np.squeeze(predicted.eq(target.data.view_as(predicted)))\n",
    "        for i in range(len(target.data)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "  model.apply(weight_reset)\n",
    "\n",
    "  print('Test Accuracy (Total): %.2f %% (%d/%d)\\n' % (\n",
    "      100. * np.sum(class_correct) / np.sum(class_total),\n",
    "      np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b0 , smooth labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 1.83452 valid_loss: 1.72721\n",
      "Test Accuracy (Total): 86.86 % (1448/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 1.97010 valid_loss: 1.69423\n",
      "Test Accuracy (Total): 87.82 % (1464/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   3/1000] train_loss: 1.88279 valid_loss: 1.68182\n",
      "Test Accuracy (Total): 88.12 % (1469/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   4/1000] train_loss: 1.85174 valid_loss: 1.67763\n",
      "Test Accuracy (Total): 88.18 % (1470/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   5/1000] train_loss: 1.79017 valid_loss: 1.66954\n",
      "Test Accuracy (Total): 88.66 % (1478/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   6/1000] train_loss: 1.76670 valid_loss: 1.66932\n",
      "Test Accuracy (Total): 88.84 % (1481/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   7/1000] train_loss: 1.74265 valid_loss: 1.66351\n",
      "Test Accuracy (Total): 89.20 % (1487/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   8/1000] train_loss: 1.73055 valid_loss: 1.67022\n",
      "Test Accuracy (Total): 89.08 % (1485/1667)\n",
      "\n",
      "[   9/1000] train_loss: 1.71022 valid_loss: 1.66880\n",
      "Test Accuracy (Total): 88.72 % (1479/1667)\n",
      "\n",
      "[  10/1000] train_loss: 1.68700 valid_loss: 1.67560\n",
      "Test Accuracy (Total): 88.60 % (1477/1667)\n",
      "\n",
      "[  11/1000] train_loss: 1.67809 valid_loss: 1.67610\n",
      "Test Accuracy (Total): 88.78 % (1480/1667)\n",
      "\n",
      "[  12/1000] train_loss: 1.67764 valid_loss: 1.66274\n",
      "Test Accuracy (Total): 88.78 % (1480/1667)\n",
      "\n",
      "Saving Model...\n",
      "[  13/1000] train_loss: 1.65933 valid_loss: 1.67483\n",
      "Test Accuracy (Total): 88.60 % (1477/1667)\n",
      "\n",
      "[  14/1000] train_loss: 1.64999 valid_loss: 1.67165\n",
      "Test Accuracy (Total): 88.54 % (1476/1667)\n",
      "\n",
      "[  15/1000] train_loss: 1.64379 valid_loss: 1.67782\n",
      "Test Accuracy (Total): 88.12 % (1469/1667)\n",
      "\n",
      "[  16/1000] train_loss: 1.64856 valid_loss: 1.67366\n",
      "Test Accuracy (Total): 87.82 % (1464/1667)\n",
      "\n",
      "[  17/1000] train_loss: 1.63198 valid_loss: 1.67770\n",
      "Test Accuracy (Total): 88.90 % (1482/1667)\n",
      "\n",
      "[  18/1000] train_loss: 1.63143 valid_loss: 1.67714\n",
      "Test Accuracy (Total): 88.54 % (1476/1667)\n",
      "\n",
      "[  19/1000] train_loss: 1.62878 valid_loss: 1.68462\n",
      "Test Accuracy (Total): 88.42 % (1474/1667)\n",
      "\n",
      "[  20/1000] train_loss: 1.64154 valid_loss: 1.68234\n",
      "Test Accuracy (Total): 88.36 % (1473/1667)\n",
      "\n",
      "[  21/1000] train_loss: 1.61546 valid_loss: 1.68699\n",
      "Test Accuracy (Total): 88.42 % (1474/1667)\n",
      "\n",
      "[  22/1000] train_loss: 1.61245 valid_loss: 1.67788\n",
      "Test Accuracy (Total): 87.88 % (1465/1667)\n",
      "\n",
      "[  23/1000] train_loss: 1.61360 valid_loss: 1.68489\n",
      "Test Accuracy (Total): 87.94 % (1466/1667)\n",
      "\n",
      "[  24/1000] train_loss: 1.61692 valid_loss: 1.68442\n",
      "Test Accuracy (Total): 88.06 % (1468/1667)\n",
      "\n",
      "[  25/1000] train_loss: 1.61874 valid_loss: 1.68904\n",
      "Test Accuracy (Total): 87.52 % (1459/1667)\n",
      "\n",
      "[  26/1000] train_loss: 1.61462 valid_loss: 1.69338\n",
      "Test Accuracy (Total): 87.76 % (1463/1667)\n",
      "\n",
      "[  27/1000] train_loss: 1.62403 valid_loss: 1.69358\n",
      "Test Accuracy (Total): 88.18 % (1470/1667)\n",
      "\n",
      "Early stopping\n",
      "Test Accuracy (Total): 8.10 % (135/1667)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b0 resize 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 2.04568 valid_loss: 0.99604\n",
      "Test Accuracy (Total): 75.94 % (1266/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 1.32789 valid_loss: 0.61501\n",
      "Test Accuracy (Total): 84.22 % (1404/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   3/1000] train_loss: 1.08011 valid_loss: 0.52910\n",
      "Test Accuracy (Total): 85.84 % (1431/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   4/1000] train_loss: 0.91598 valid_loss: 0.49725\n",
      "Test Accuracy (Total): 86.68 % (1445/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   5/1000] train_loss: 0.80801 valid_loss: 0.45589\n",
      "Test Accuracy (Total): 86.14 % (1436/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   6/1000] train_loss: 0.72587 valid_loss: 0.45264\n",
      "Test Accuracy (Total): 87.10 % (1452/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   7/1000] train_loss: 0.65294 valid_loss: 0.41709\n",
      "Test Accuracy (Total): 87.82 % (1464/1667)\n",
      "\n",
      "Saving Model...\n",
      "[   8/1000] train_loss: 0.60336 valid_loss: 0.43309\n",
      "Test Accuracy (Total): 88.48 % (1475/1667)\n",
      "\n",
      "[   9/1000] train_loss: 0.55257 valid_loss: 0.44006\n",
      "Test Accuracy (Total): 87.82 % (1464/1667)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e261fbce5f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-de97f00031db>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, patience, num_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#scheduler.step() #내가 추가한거\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 26167, 26168, 26169, 26170, 26171, 26172, 26173, 26174, 26175, 26176, 26177, 26178, 26179, 26180, 26181, 26182, 26183, 26184, 26185, 26186, 26187, 26188, 26189, 26190, 26191, 26192, 26193, 26194, 26195, 26196, 26197, 26198, 26199, 26200, 26201, 26202, 26203, 26204, 26205, 26206, 26207, 26208, 26209, 26210, 26211, 26212, 26213, 26214, 26215, 26216, 26217, 26218, 26219, 26220, 26221, 26222, 26223, 26224, 26225, 26226, 26227, 26228, 26229, 26230) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e261fbce5f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-de97f00031db>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, patience, num_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/timm/data/loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 26167, 26168, 26169, 26170, 26171, 26172, 26173, 26174, 26175, 26176, 26177, 26178, 26179, 26180, 26181, 26182, 26183, 26184, 26185, 26186, 26187, 26188, 26189, 26190, 26191, 26192, 26193, 26194, 26195, 26196, 26197, 26198, 26199, 26200, 26201, 26202, 26203, 26204, 26205, 26206, 26207, 26208, 26209, 26210, 26211, 26212, 26213, 26214, 26215, 26216, 26217, 26218, 26219, 26220, 26221, 26222, 26223, 26224, 26225, 26226, 26227, 26228, 26229, 26230) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize(600)\n",
    "b7/finetuning 없음 (batch size 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 4.09283 valid_loss: 3.85271\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 3.50324 valid_loss: 4.67777\n",
      "[   3/1000] train_loss: 3.26275 valid_loss: 4.68264\n",
      "[   4/1000] train_loss: 3.12964 valid_loss: 5.53166\n",
      "[   5/1000] train_loss: 3.01338 valid_loss: 5.95381\n",
      "[   6/1000] train_loss: 2.95209 valid_loss: 6.44378\n",
      "Early stopping\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-066e498ed3bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b77d8604be6a>\u001b[0m in \u001b[0;36mprint_test_accuracy\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mclass_correct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mclass_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 5\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resize(256) 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 3.68471 valid_loss: 3.19933\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 3.05829 valid_loss: 3.11481\n",
      "Saving Model...\n",
      "[   3/1000] train_loss: 2.90352 valid_loss: 3.11834\n",
      "[   4/1000] train_loss: 2.84723 valid_loss: 3.04560\n",
      "Saving Model...\n",
      "[   5/1000] train_loss: 2.80475 valid_loss: 3.06946\n",
      "[   6/1000] train_loss: 2.78334 valid_loss: 3.04708\n",
      "[   7/1000] train_loss: 2.76857 valid_loss: 3.08668\n",
      "[   8/1000] train_loss: 2.77765 valid_loss: 3.10754\n",
      "[   9/1000] train_loss: 2.76548 valid_loss: 3.09385\n",
      "[  10/1000] train_loss: 2.76035 valid_loss: 3.12681\n",
      "[  11/1000] train_loss: 2.76435 valid_loss: 3.08634\n",
      "[  12/1000] train_loss: 2.76891 valid_loss: 3.09001\n",
      "[  13/1000] train_loss: 2.73274 valid_loss: 3.08450\n",
      "[  14/1000] train_loss: 2.73949 valid_loss: 3.15281\n",
      "[  15/1000] train_loss: 2.80030 valid_loss: 3.24563\n",
      "[  16/1000] train_loss: 2.75570 valid_loss: 3.11533\n",
      "[  17/1000] train_loss: 2.72768 valid_loss: 3.13789\n",
      "[  18/1000] train_loss: 2.71999 valid_loss: 3.08311\n",
      "[  19/1000] train_loss: 2.71517 valid_loss: 3.11860\n",
      "Early stopping\n",
      "Test Accuracy (Total): 87.28 % (1455/1667)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models.resnext101_32x8d(pretrained=True, progress = True).to('cuda')  쓴거\n",
    "rl = 0.001\n",
    "batchsize = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 2.49489 valid_loss: 3.02104\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 1.22229 valid_loss: 1.57715\n",
      "Saving Model...\n",
      "[   3/1000] train_loss: 0.87225 valid_loss: 1.44397\n",
      "Saving Model...\n",
      "[   4/1000] train_loss: 0.63376 valid_loss: 1.52186\n",
      "[   5/1000] train_loss: 0.50805 valid_loss: 1.49858\n",
      "[   6/1000] train_loss: 0.44530 valid_loss: 2.06596\n",
      "[   7/1000] train_loss: 0.40726 valid_loss: 1.71300\n",
      "[   8/1000] train_loss: 0.26045 valid_loss: 1.47815\n",
      "[   9/1000] train_loss: 0.20722 valid_loss: 2.00285\n",
      "[  10/1000] train_loss: 0.22510 valid_loss: 1.99222\n",
      "[  11/1000] train_loss: 0.36936 valid_loss: 1.76069\n",
      "[  12/1000] train_loss: 0.89980 valid_loss: 1.68771\n",
      "[  13/1000] train_loss: 0.41474 valid_loss: 1.48265\n",
      "[  14/1000] train_loss: 0.18906 valid_loss: 2.00617\n",
      "[  15/1000] train_loss: 0.18387 valid_loss: 1.58351\n",
      "[  16/1000] train_loss: 0.12665 valid_loss: 1.72464\n",
      "[  17/1000] train_loss: 0.09174 valid_loss: 1.64384\n",
      "[  18/1000] train_loss: 0.07164 valid_loss: 1.74159\n",
      "Early stopping\n",
      "Test Accuracy (Total): 60.29 % (1005/1667)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models.resnext101_32x8d(pretrained=True, progress = True).to('cuda')  쓴거\n",
    "label smoothing = 0.3 해본거 뭔지 모르겠음.  (https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch)\n",
    "nvidat 뭐시기 씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 4.98006 valid_loss: 3.78770\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 3.42048 valid_loss: 3.47698\n",
      "Saving Model...\n",
      "[   3/1000] train_loss: 3.06916 valid_loss: 3.40553\n",
      "Saving Model...\n",
      "[   4/1000] train_loss: 2.92167 valid_loss: 3.36669\n",
      "Saving Model...\n",
      "[   5/1000] train_loss: 2.83931 valid_loss: 3.33530\n",
      "Saving Model...\n",
      "[   6/1000] train_loss: 2.79113 valid_loss: 3.32254\n",
      "Saving Model...\n",
      "[   7/1000] train_loss: 2.76962 valid_loss: 3.32636\n",
      "[   8/1000] train_loss: 2.74539 valid_loss: 3.31648\n",
      "Saving Model...\n",
      "[   9/1000] train_loss: 2.72877 valid_loss: 3.34809\n",
      "[  10/1000] train_loss: 2.72195 valid_loss: 3.31191\n",
      "Saving Model...\n",
      "[  11/1000] train_loss: 2.71174 valid_loss: 3.31319\n",
      "[  12/1000] train_loss: 2.70807 valid_loss: 3.31754\n",
      "[  13/1000] train_loss: 2.70332 valid_loss: 3.33277\n",
      "[  14/1000] train_loss: 2.69845 valid_loss: 3.32785\n",
      "[  15/1000] train_loss: 2.69757 valid_loss: 3.32530\n",
      "[  16/1000] train_loss: 2.69613 valid_loss: 3.33687\n",
      "[  17/1000] train_loss: 2.69852 valid_loss: 3.35525\n",
      "[  18/1000] train_loss: 2.69970 valid_loss: 3.35167\n",
      "[  19/1000] train_loss: 2.70039 valid_loss: 3.36760\n",
      "[  20/1000] train_loss: 2.69725 valid_loss: 3.40727\n",
      "[  21/1000] train_loss: 2.71481 valid_loss: 3.40768\n",
      "[  22/1000] train_loss: 2.72726 valid_loss: 3.42465\n",
      "[  23/1000] train_loss: 2.74664 valid_loss: 3.40456\n",
      "[  24/1000] train_loss: 2.77523 valid_loss: 3.43106\n",
      "[  25/1000] train_loss: 2.76311 valid_loss: 3.41005\n",
      "Early stopping\n",
      "Test Accuracy (Total): 75.88 % (1265/1667)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models.resnext101_32x8d(pretrained=True, progress = True).to('cuda')  쓴거\n",
    "label smoothing = 0.3 해본거 뭔지 모르겠음.  (https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch)\n",
    "nvidat 뭐시기 씀  \n",
    "+learning rate schedule : exponetial 써봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 5.54712 valid_loss: 4.67650\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 4.30602 valid_loss: 4.47192\n",
      "Saving Model...\n",
      "[   3/1000] train_loss: 4.24488 valid_loss: 4.46818\n",
      "Saving Model...\n",
      "[   4/1000] train_loss: 4.23627 valid_loss: 4.45199\n",
      "Saving Model...\n",
      "[   5/1000] train_loss: 4.24401 valid_loss: 4.42681\n",
      "Saving Model...\n",
      "[   6/1000] train_loss: 4.24099 valid_loss: 4.44410\n",
      "[   7/1000] train_loss: 4.24283 valid_loss: 4.47978\n",
      "[   8/1000] train_loss: 4.24101 valid_loss: 4.46950\n",
      "[   9/1000] train_loss: 4.23962 valid_loss: 4.43817\n",
      "[  10/1000] train_loss: 4.23426 valid_loss: 4.45474\n",
      "[  11/1000] train_loss: 4.24901 valid_loss: 4.45258\n",
      "[  12/1000] train_loss: 4.24028 valid_loss: 4.44296\n",
      "[  13/1000] train_loss: 4.24799 valid_loss: 4.46802\n",
      "[  14/1000] train_loss: 4.24406 valid_loss: 4.47880\n",
      "[  15/1000] train_loss: 4.24200 valid_loss: 4.42268\n",
      "Saving Model...\n",
      "[  16/1000] train_loss: 4.23380 valid_loss: 4.41957\n",
      "Saving Model...\n",
      "[  17/1000] train_loss: 4.23507 valid_loss: 4.44467\n",
      "[  18/1000] train_loss: 4.24045 valid_loss: 4.44792\n",
      "[  19/1000] train_loss: 4.25025 valid_loss: 4.45034\n",
      "[  20/1000] train_loss: 4.25288 valid_loss: 4.45458\n",
      "[  21/1000] train_loss: 4.24418 valid_loss: 4.45448\n",
      "[  22/1000] train_loss: 4.23225 valid_loss: 4.45900\n",
      "[  23/1000] train_loss: 4.24655 valid_loss: 4.42036\n",
      "[  24/1000] train_loss: 4.23752 valid_loss: 4.44226\n",
      "[  25/1000] train_loss: 4.24466 valid_loss: 4.42348\n",
      "[  26/1000] train_loss: 4.26299 valid_loss: 4.47990\n",
      "[  27/1000] train_loss: 4.24125 valid_loss: 4.46559\n",
      "[  28/1000] train_loss: 4.24188 valid_loss: 4.45180\n",
      "[  29/1000] train_loss: 4.24101 valid_loss: 4.47491\n",
      "[  30/1000] train_loss: 4.25275 valid_loss: 4.45396\n",
      "[  31/1000] train_loss: 4.24606 valid_loss: 4.45688\n",
      "Early stopping\n",
      "Test Accuracy (Total): 38.21 % (637/1667)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models.resnext101_32x8d(pretrained=True, progress = True).to('cuda')  쓴거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 3.34607 valid_loss: 1.29235\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 0.79014 valid_loss: 0.97661\n",
      "Saving Model...\n",
      "[   3/1000] train_loss: 0.30339 valid_loss: 0.99960\n",
      "[   4/1000] train_loss: 0.15766 valid_loss: 1.06899\n",
      "[   5/1000] train_loss: 0.08559 valid_loss: 1.18780\n",
      "[   6/1000] train_loss: 0.08409 valid_loss: 1.13143\n",
      "[   7/1000] train_loss: 0.05148 valid_loss: 1.23776\n",
      "[   8/1000] train_loss: 0.04816 valid_loss: 1.19748\n",
      "[   9/1000] train_loss: 0.04579 valid_loss: 1.23585\n",
      "[  10/1000] train_loss: 0.03984 valid_loss: 1.32761\n",
      "[  11/1000] train_loss: 0.05532 valid_loss: 1.31975\n",
      "[  12/1000] train_loss: 0.05877 valid_loss: 1.26559\n",
      "[  13/1000] train_loss: 0.05935 valid_loss: 1.26448\n",
      "[  14/1000] train_loss: 0.06302 valid_loss: 1.26687\n",
      "[  15/1000] train_loss: 0.04225 valid_loss: 1.23406\n",
      "[  16/1000] train_loss: 0.03199 valid_loss: 1.36592\n",
      "[  17/1000] train_loss: 0.03877 valid_loss: 1.40870\n",
      "Early stopping\n",
      "Test Accuracy (Total): 71.21 % (1187/1667)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models.resnext101_32x8d(pretrained=False, progress = True).to('cuda')  쓴거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 3.63468 valid_loss: 3.24151\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 2.99377 valid_loss: 2.98866\n",
      "Saving Model...\n",
      "[   3/1000] train_loss: 2.95420 valid_loss: 2.93517\n",
      "Saving Model...\n",
      "[   4/1000] train_loss: 2.89297 valid_loss: 2.89309\n",
      "Saving Model...\n",
      "[   5/1000] train_loss: 2.82801 valid_loss: 2.81104\n",
      "Saving Model...\n",
      "[   6/1000] train_loss: 2.77810 valid_loss: 2.75461\n",
      "Saving Model...\n",
      "[   7/1000] train_loss: 2.71515 valid_loss: 2.74266\n",
      "Saving Model...\n",
      "[   8/1000] train_loss: 2.67226 valid_loss: 2.75661\n",
      "[   9/1000] train_loss: 2.60114 valid_loss: 2.65724\n",
      "Saving Model...\n",
      "[  10/1000] train_loss: 2.54870 valid_loss: 2.69271\n",
      "[  11/1000] train_loss: 2.49091 valid_loss: 2.64498\n",
      "Saving Model...\n",
      "[  12/1000] train_loss: 2.41394 valid_loss: 2.65417\n",
      "[  13/1000] train_loss: 2.36246 valid_loss: 2.70276\n",
      "[  14/1000] train_loss: 2.34232 valid_loss: 2.68470\n",
      "[  15/1000] train_loss: 2.23078 valid_loss: 2.65275\n",
      "[  16/1000] train_loss: 2.13045 valid_loss: 2.65005\n",
      "[  17/1000] train_loss: 2.10029 valid_loss: 2.60670\n",
      "Saving Model...\n",
      "[  18/1000] train_loss: 1.97107 valid_loss: 2.77918\n",
      "[  19/1000] train_loss: 1.92719 valid_loss: 2.72385\n",
      "[  20/1000] train_loss: 1.83150 valid_loss: 2.64171\n",
      "[  21/1000] train_loss: 1.74603 valid_loss: 2.71420\n",
      "[  22/1000] train_loss: 1.60254 valid_loss: 3.02041\n",
      "[  23/1000] train_loss: 1.47889 valid_loss: 2.84757\n",
      "[  24/1000] train_loss: 1.31794 valid_loss: 3.04439\n",
      "[  25/1000] train_loss: 1.24438 valid_loss: 2.94389\n",
      "[  26/1000] train_loss: 1.20022 valid_loss: 3.03936\n",
      "[  27/1000] train_loss: 1.08535 valid_loss: 3.08368\n",
      "[  28/1000] train_loss: 0.98392 valid_loss: 3.28250\n",
      "[  29/1000] train_loss: 0.92618 valid_loss: 3.31241\n",
      "[  30/1000] train_loss: 0.82430 valid_loss: 3.40466\n",
      "[  31/1000] train_loss: 0.79393 valid_loss: 3.42106\n",
      "[  32/1000] train_loss: 0.68473 valid_loss: 3.57664\n",
      "Early stopping\n",
      "Test Accuracy (Total): 22.98 % (383/1667)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models.wide_resnet50_2(pretrained=False).to('cuda') 쓴거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 3.57445 valid_loss: 2.95327\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 2.83149 valid_loss: 2.75759\n",
      "Saving Model...\n",
      "[   3/1000] train_loss: 2.65120 valid_loss: 2.66315\n",
      "Saving Model...\n",
      "[   4/1000] train_loss: 2.51122 valid_loss: 2.56807\n",
      "Saving Model...\n",
      "[   5/1000] train_loss: 2.35330 valid_loss: 2.57100\n",
      "[   6/1000] train_loss: 2.24620 valid_loss: 2.48517\n",
      "Saving Model...\n",
      "[   7/1000] train_loss: 2.14792 valid_loss: 2.47959\n",
      "Saving Model...\n",
      "[   8/1000] train_loss: 1.97883 valid_loss: 2.61383\n",
      "[   9/1000] train_loss: 1.82066 valid_loss: 2.62226\n",
      "[  10/1000] train_loss: 1.61720 valid_loss: 2.65290\n",
      "[  11/1000] train_loss: 1.46929 valid_loss: 2.57710\n",
      "[  12/1000] train_loss: 1.28618 valid_loss: 2.87399\n",
      "[  13/1000] train_loss: 1.13475 valid_loss: 2.91118\n",
      "[  14/1000] train_loss: 0.94653 valid_loss: 3.01158\n",
      "[  15/1000] train_loss: 0.83789 valid_loss: 3.18958\n",
      "[  16/1000] train_loss: 0.70916 valid_loss: 3.17891\n",
      "[  17/1000] train_loss: 0.58533 valid_loss: 3.20930\n",
      "[  18/1000] train_loss: 0.51460 valid_loss: 3.33773\n",
      "[  19/1000] train_loss: 0.40903 valid_loss: 3.64026\n",
      "[  20/1000] train_loss: 0.38744 valid_loss: 3.61108\n",
      "[  21/1000] train_loss: 0.37133 valid_loss: 3.69531\n",
      "[  22/1000] train_loss: 0.33033 valid_loss: 3.78680\n",
      "Early stopping\n",
      "Test Accuracy (Total): 24.78 % (413/1667)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models.wide_resnet50_2(pretrained=True).to('cuda') 쓴거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1/1000] train_loss: 6.23711 valid_loss: 2.49377\n",
      "Saving Model...\n",
      "[   2/1000] train_loss: 1.34758 valid_loss: 1.59409\n",
      "Saving Model...\n",
      "[   3/1000] train_loss: 0.63932 valid_loss: 1.44614\n",
      "Saving Model...\n",
      "[   4/1000] train_loss: 0.29878 valid_loss: 1.42573\n",
      "Saving Model...\n",
      "[   5/1000] train_loss: 0.16961 valid_loss: 1.38099\n",
      "Saving Model...\n",
      "[   6/1000] train_loss: 0.09423 valid_loss: 1.40760\n",
      "[   7/1000] train_loss: 0.05140 valid_loss: 1.43718\n",
      "[   8/1000] train_loss: 0.03289 valid_loss: 1.50548\n",
      "[   9/1000] train_loss: 0.02314 valid_loss: 1.46350\n",
      "[  10/1000] train_loss: 0.01942 valid_loss: 1.44684\n",
      "[  11/1000] train_loss: 0.01234 valid_loss: 1.47245\n",
      "[  12/1000] train_loss: 0.00733 valid_loss: 1.48660\n",
      "[  13/1000] train_loss: 0.00534 valid_loss: 1.51482\n",
      "[  14/1000] train_loss: 0.00493 valid_loss: 1.51713\n",
      "[  15/1000] train_loss: 0.00627 valid_loss: 1.54443\n",
      "[  16/1000] train_loss: 0.00427 valid_loss: 1.58538\n",
      "[  17/1000] train_loss: 0.00418 valid_loss: 1.57446\n",
      "[  18/1000] train_loss: 0.00300 valid_loss: 1.57757\n",
      "[  19/1000] train_loss: 0.00443 valid_loss: 1.55026\n",
      "[  20/1000] train_loss: 0.00359 valid_loss: 1.58548\n",
      "Early stopping\n",
      "Test Accuracy (Total): 65.63 % (1094/1667)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 15\n",
    "\n",
    "final_model, train_losses, valid_losses= train_model(model, patience, num_epochs)\n",
    "print_test_accuracy(final_model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
